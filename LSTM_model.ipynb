{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0504ce19",
   "metadata": {},
   "source": [
    "# Predicting the Next Token in Tweets Using LSTM and TensorFlow\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "7354dc75",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Embedding, LSTM, Dense\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "import numpy as np\n",
    "import pandas as pd\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c226d827",
   "metadata": {},
   "source": [
    "## 2. Load and Prepare Data\n",
    "\n",
    "Next, we load the preprocessed tweets and prepare them for training. This involves tokenizing the text data and creating sequences that will be used as input to our LSTM model.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "71285932",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('preprocessed_tweets.csv')\n",
    "\n",
    "df['tweet'] = df['tokens'].apply(lambda x: ' '.join(eval(x)))\n",
    "\n",
    "tokenizer = Tokenizer()\n",
    "tokenizer.fit_on_texts(df['tweet'])\n",
    "total_words = len(tokenizer.word_index) + 1\n",
    "\n",
    "input_sequences = []\n",
    "for line in df['tweet']:\n",
    "    token_list = tokenizer.texts_to_sequences([line])[0]\n",
    "    for i in range(1, len(token_list)):\n",
    "        n_gram_sequence = token_list[:i+1]\n",
    "        input_sequences.append(n_gram_sequence)\n",
    "\n",
    "max_sequence_len = max([len(x) for x in input_sequences])\n",
    "input_sequences = np.array(pad_sequences(input_sequences, maxlen=max_sequence_len, padding='pre'))\n",
    "\n",
    "predictors, label = input_sequences[:,:-1],input_sequences[:,-1]\n",
    "label = tf.keras.utils.to_categorical(label, num_classes=total_words)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e8c364dc",
   "metadata": {},
   "source": [
    "## 3. Build the LSTM Model\n",
    "\n",
    "With our data prepared, we can now build the LSTM model. We use an Embedding layer to learn token embeddings, followed by an LSTM layer and a Dense layer for prediction.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9126632f",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Sequential()\n",
    "model.add(Embedding(total_words, 100, input_length=max_sequence_len-1))\n",
    "model.add(LSTM(150, return_sequences=True))\n",
    "model.add(LSTM(100))\n",
    "model.add(Dense(total_words, activation='softmax'))\n",
    "\n",
    "model.summary()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "37c74a3e",
   "metadata": {},
   "source": [
    "## 4. Compile the Model\n",
    "\n",
    "We compile the model using the 'adam' optimizer and 'categorical_crossentropy' as the loss function, suitable for multi-class classification tasks.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "27775f28",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e2eceed5",
   "metadata": {},
   "source": [
    "## 5. Train the Model\n",
    "\n",
    "It's time to train our model. Note that this process can be time-consuming, depending on the size of your data and the complexity of the model.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "82e4d08c",
   "metadata": {},
   "outputs": [],
   "source": [
    "history = model.fit(predictors, label, epochs=100, verbose=1)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b5116211",
   "metadata": {},
   "source": [
    "## 6. Evaluate the Model\n",
    "\n",
    "After training, we can evaluate our model's performance and plot the training history to visualize the learning process.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d8df6459",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Plot accuracy\n",
    "plt.plot(history.history['accuracy'])\n",
    "plt.title('Model Accuracy')\n",
    "plt.ylabel('accuracy')\n",
    "plt.xlabel('epoch')\n",
    "plt.legend(['train'], loc='upper left')\n",
    "plt.show()\n",
    "\n",
    "# Plot loss\n",
    "plt.plot(history.history['loss'])\n",
    "plt.title('Model Loss')\n",
    "plt.ylabel('loss')\n",
    "plt.xlabel('epoch')\n",
    "plt.legend(['train'], loc='upper left')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0914850c",
   "metadata": {},
   "source": [
    "## 7. Test the Model\n",
    "\n",
    "Finally, let's test our model with a custom input to predict the next token in a sequence.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bbf05db5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_next_token(seed_text):\n",
    "    token_list = tokenizer.texts_to_sequences([seed_text])[0]\n",
    "    token_list = pad_sequences([token_list], maxlen=max_sequence_len-1, padding='pre')\n",
    "    predicted = model.predict_classes(token_list, verbose=0)\n",
    "    return tokenizer.index_word[predicted[0]]\n",
    "\n",
    "seed_text = \"I feel\"\n",
    "next_token = predict_next_token(seed_text)\n",
    "print(f\"Next token after '{seed_text}': {next_token}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9ee283b6",
   "metadata": {},
   "source": [
    "## 8. Save the Model\n",
    "\n",
    "To deploy the model, we first need to save it. TensorFlow provides a simple API to save models in the SavedModel format, which can be easily served in different environments.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "bc23e78d",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'model' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[1], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m model_save_path \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124msaved_model/next_token_predictor\u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[0;32m----> 2\u001b[0m model\u001b[38;5;241m.\u001b[39msave(model_save_path)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'model' is not defined"
     ]
    }
   ],
   "source": [
    "model_save_path = 'saved_model/next_token_predictor'\n",
    "model.save(model_save_path)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e8a2f012",
   "metadata": {},
   "source": [
    "## 9. Containerize the Model Using Docker\n",
    "\n",
    "To prepare our model for deployment on AWS SageMaker, we'll containerize it using Docker. This process involves creating a Dockerfile, building a Docker image, and testing it locally.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "291e37da",
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "invalid syntax (2872726390.py, line 3)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;36m  Cell \u001b[0;32mIn[2], line 3\u001b[0;36m\u001b[0m\n\u001b[0;31m    FROM tensorflow/serving\u001b[0m\n\u001b[0m         ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m invalid syntax\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "FROM tensorflow/serving\n",
    "\n",
    "COPY ${model_save_path} /models/next_token_predictor/1\n",
    "\n",
    "ENV MODEL_NAME=next_token_predictor\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dac32721",
   "metadata": {},
   "outputs": [],
   "source": [
    "docker build -t next-token-predictor:latest .\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c3938bb9",
   "metadata": {},
   "outputs": [],
   "source": [
    "docker run -p 8501:8501 --name=my_model_container next-token-predictor:latest\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "97790e07",
   "metadata": {},
   "source": [
    "## 10. Upload the Model to Amazon ECR\n",
    "\n",
    "For deploying our model with AWS SageMaker, we need to upload our Docker container to Amazon Elastic Container Registry (ECR). This section outlines the steps to create a repository in ECR, authenticate Docker to push images to ECR, and finally, push the image.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5af59dc8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import boto3\n",
    "\n",
    "aws_region = 'us-west-2'\n",
    "ecr_repository_name = 'next-token-predictor'\n",
    "\n",
    "ecr_client = boto3.client('ecr', region_name=aws_region)\n",
    "\n",
    "response = ecr_client.create_repository(repositoryName=ecr_repository_name)\n",
    "repository_uri = response['repository']['repositoryUri']\n",
    "\n",
    "print(f\"Repository URI: {repository_uri}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6159c28a",
   "metadata": {},
   "outputs": [],
   "source": [
    "aws ecr get-login-password --region us-west-2 | docker login --username AWS --password-stdin <repository_uri>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4062ad7f",
   "metadata": {},
   "outputs": [],
   "source": [
    "docker tag next-token-predictor:latest <repository_uri>:latest\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "136a8154",
   "metadata": {},
   "outputs": [],
   "source": [
    "docker push <repository_uri>:latest\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
